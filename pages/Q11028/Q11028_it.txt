In generale può essere descritta come informazione qualsiasi "notizia, dato o elemento che consente di avere conoscenza più o meno esatta di fatti, situazioni, modi di essere".
Nella pratica il concetto di informazione non è ben definito e tende ad assumere sfumature diverse in base al contesto di applicazione.
In biblioteconomia è comune definire l'informazione come l'insieme di dati, correlati tra loro, con cui un'idea (o un fatto) prende forma ed è comunicata.
In ingegneria ed informatica si è invece consolidata una teoria quantitativa dell'informazione, che pur essendo costruita con rigore e formalismo matematico poggia comunque su una definizione assiomatica e circolare del concetto.
In ogni caso l'informazione è un concetto trascendente, il cui studio risulta di notevole interesse in numerosi ambiti: ad esempio in campo tecnico è oggetto di studio dell'ingegneria dell'informazione, sul fronte delle scienze sociali è oggetto d'indagine delle scienze della comunicazione e in generale della sociologia, con particolare riguardo agli aspetti legati alla diffusione dei mezzi di comunicazione di massa nell'attuale società dell'informazione (o era dell'informazione).


== Etimologia ==
La parola informazione deriva dal sostantivo latino informatio(-nis) (dal verbo informare, nel significato di "dare forma", "disciplinare", "istruire", "insegnare") . Già in latino la parola veniva usata anche per indicare un "concetto" o un'"idea", ma non è chiaro se questa parola possa avere influenzato lo sviluppo della parola informazione.
Inoltre la parola greca corrispondente era "μορφή" (morfè, da cui il latino forma per metatesi), oppure "εἶδος" (éidos, da cui il latino idea), cioè "idea", "concetto" o "forma", "immagine"; la seconda parola fu notoriamente usata tecnicamente in ambito filosofico da Platone e Aristotele per indicare l'identità ideale o essenza di qualcosa (vedi Teoria delle forme). Eidos si può anche associare a "pensiero", "asserzione" o "concetto".


== Descrizione ==
In generale un'informazione ha valore in quanto potenzialmente utile al fruitore per i suoi molteplici scopi: nell'informazione, infatti, è spesso contenuta conoscenza o esperienza di fatti reali vissuti da altri soggetti e che possono risultare utili senza dover necessariamente attendere di sperimentare ognuno ogni determinata situazione. Sotto questo punto di vista il concetto utile di informazione e la parallela necessità di comunicare o scambiare informazione tra individui nasce, nella storia dell'umanità, con l'elaborazione di codici condivisi come il linguaggio e  la successiva invenzione della scrittura come mezzo per tramandare l'informazione ai posteri. Secondo quest'ottica la storia e l'evoluzione della società umana sono frutto dell'accumulazione di conoscenza sotto forma di informazione. Nell'informazione ad esempio è contenuto know how utile per eseguire una determinata attività o compito, cosa che la rende ad esempio una risorsa strategica in ambito economico dell'economia aziendale.
L'informazione e la sua elaborazione attraverso i computer hanno avuto certamente un impatto notevole nella nostra attuale vita quotidiana. L'importanza è testimoniata, ad esempio, dai sistemi di protezione escogitati mediante la crittografia e dal valore commerciale della borsa tecnologica. L'uso appropriato dell'informazione pone anche problemi etici di rilievo, come nel caso della riservatezza riguardo alle informazioni cliniche che potrebbero altrimenti avvantaggiare le compagnie di assicurazioni mediche e danneggiare i pazienti.
L'importanza e la diffusione dell'informazione nella società moderna è tale che a questa spesso ci si riferisce come la Società dell'Informazione.


== Nei vari contesti ==

Col progredire delle conoscenze umane l'ingegneria dell'informazione ha trovato applicazioni sempre più vaste e differenziate.
I dati in un archivio sono informazioni, ma anche la configurazione degli atomi di un gas può venire considerata informazione. L'informazione può essere quindi misurata come le altre entità fisiche ed è sempre esistita, anche se la sua importanza è stata riconosciuta solo nel XX secolo.
Per esempio, la fondamentale scoperta della “doppia elica” del DNA nel 1953 da parte di James Watson e Francis Crick ha posto le basi biologiche per la comprensione della struttura degli esseri viventi da un punto di vista informativo. La doppia elica è costituita da due filamenti accoppiati e avvolti su se stessi, a formare una struttura elicoidale tridimensionale. Ciascun filamento può essere ricondotto a una sequenza di acidi nucleici (adenina, citosina, guanina, timina). Per rappresentarlo, si usa un alfabeto finito come nei calcolatori, quaternario invece che binario, dove le lettere sono scelte tra A, C, G e T, le iniziali delle quattro componenti fondamentali. Il DNA rappresenta quindi il contenuto informativo delle funzionalità e della struttura degli esseri viventi.
Altre definizioni provengono dall'informatica e dalla telematica:

Nel modello di Shannon e Weaver, l'informazione è considerata parte integrante del processo comunicativo;
La teoria dell'informazione ha come scopo quello di fornire metodi per comprimere al massimo l'informazione prodotta da una sorgente eliminando tutta la ridondanza.


== Aspetti tecnici ==

L'informazione è generalmente associata a segnali, trasmissibili da un sistema di telecomunicazioni e memorizzabili su supporti di memorizzazione.


=== Misurazione ===

Secondo la Teoria dell'Informazione in una comunicazione, che avviene attraverso un dato alfabeto di simboli, l'informazione viene associata a ciascun simbolo trasmesso e viene definita come la riduzione di incertezza che si poteva avere a priori sul simbolo trasmesso.
In particolare, la quantità di informazione collegata a un simbolo è definita come

  
    
      
        I
        =
        −
        
          log
          
            2
          
        
        ⁡
        
          P
          
            i
          
        
      
    
    {\displaystyle I=-\log _{2}P_{i}}
  

dove 
  
    
      
        
          P
          
            i
          
        
      
    
    {\displaystyle P_{i}}
  
 è la probabilità di trasmissione di quel simbolo. La quantità di informazione associata a un simbolo è misurata in bit. La quantità di informazione così definita è una variabile aleatoria discreta, il cui valor medio, tipicamente riferito alla sorgente di simboli, è detto entropia della sorgente, misurata in bit/simbolo. La velocità di informazione di una sorgente, che non coincide con la frequenza di emissione dei simboli, dato che non è detto che ogni simbolo trasporti un bit di informazione "utile", è il prodotto dell'entropia dei simboli emessi dalla sorgente per la frequenza di emissione di tali simboli (velocità di segnalazione).
Quanto sopra può essere generalizzato considerando che non è assolutamente obbligatorio che ogni simbolo sia codificato in maniera binaria (anche se questo è ciò che accade più spesso). Quindi l'informazione collegata a un simbolo codificato in base 
  
    
      
        a
      
    
    {\displaystyle a}
  
 è per definizione pari a

  
    
      
        
          I
          
            a
          
        
        =
        −
        
          log
          
            a
          
        
        ⁡
        
          P
          
            i
          
        
      
    
    {\displaystyle I_{a}=-\log _{a}P_{i}}
  

con 
  
    
      
        
          P
          
            i
          
        
      
    
    {\displaystyle P_{i}}
  
 pari alla probabilità di trasmissione associata a quel simbolo. L'entropia della sorgente è per definizione pari alla sommatoria, estesa a tutti i simboli della sorgente, dei prodotti tra la probabilità di ciascun simbolo e il suo contenuto informativo.  Nei casi particolari in cui 
  
    
      
        a
      
    
    {\displaystyle a}
  
 sia 10 l'entropia della sorgente è misurata in hartley, se invece 
  
    
      
        a
      
    
    {\displaystyle a}
  
 è pari al Numero di Eulero 
  
    
      
        e
      
    
    {\displaystyle e}
  
 si misura in nat.
Dalla formula si evince che se la probabilità 
  
    
      
        
          P
          
            i
          
        
      
    
    {\displaystyle P_{i}}
  
 di trasmettere il simbolo è pari a uno, la quantità di informazione associata è nulla; viceversa se nel caso limite ideale di 
  
    
      
        
          P
          
            i
          
        
        =
        0
      
    
    {\displaystyle P_{i}=0}
  
 la quantità di informazione sarebbe infinita. Ciò vuol dire in sostanza che tanto più un simbolo è probabile tanto meno informazione esso trasporta e viceversa: un segnale costante o uguale a se stesso non porta con sé alcuna nuova informazione essendo sempre il medesimo: si dice allora che l'informazione viaggia sotto forma di Innovazione. I segnali che trasportano informazione non sono dunque segnali deterministici, ma processi stocastici. Nella teoria dei segnali e della trasmissione questa informazione affidata a processi aleatori è la modulante (in ampiezza, fase o frequenza) di portanti fisiche tipicamente sinusoidali che traslano poi in banda il segnale informativo.


=== Rappresentazione e stoccaggio ===


=== Trasmissione ===


=== Elaborazione ===


== Altre teorie dell'informazione ==
Lo storico Yuval Noah Harari distingue tre visioni dell' informazione:

la visione ingenua dell'informazione, mutuata dal concetto psicologico di realismo ingenuo, presuppone che: << raccogliendo ed elaborando molte più informazioni di quanto possano fare i singoli individui, le grandi reti raggiungono una migliore comprensione della medicina, della fisica, dell'economia e di numerosi altri campi del sapere, e questo rende la rete non solo potente ma anche sapiente.>>.
la visione populista dell'informazione, per la quale l'informazione è solo un'arma a servizio del potere.
la visione complessa dell'informazione, presuppone che le reti informative per prosperare devono contemporaneamente divulgare la verità e creare ordine, due cose spesso in antitesi.
Il fisico computazionale Federico Faggin , inventore della tecnologia MOS  , distingue nettamente  l'informazione di tipo tecnologico informatico, dipendente dall'uomo e dal silicio, che è basata su gestione di dati attraverso la modalità meccanicistica classica, dall'informazione di tipo biologico, dipendente dal carbonio, che invece si basa su modalità quantistiche probabilistiche, che chiama informazione viva, attinente organismi individualmente unici, capaci di evolvere fisicamente, con consapevolezza di sé ovvero coscienza.


== Note ==


== Bibliografia ==
 Hans Christian von Baeyer, Informazione. Il nuovo linguaggio della scienza, Edizioni Dedalo, 2005, ISBN 978-88-220-0226-6.
 Antonio Teti, Il potere delle informazioni. Comunicazione globale, Cyberspazio, Intelligence della conoscenza, Il Sole 24 Ore Editore, 2012, ISBN 978-88-6345-446-8.
(EN) William Aspray, The Scientific Conceptualization of Information: A survey (PDF), in Annals of History of Computing, vol. 7, n. 2, Minneapolis–Saint Paul (MN, USA), American Federation of Information Processing Societies, Aprile 1985, DOI:10.1109/MAHC.1985.10018, ISSN 0164-1239 (archiviato dall'url originale il 5 ottobre 2016).
(EN) Xu Jianfeng, Wang Shuliang, Liu Zhenyu, Yashi Wang, Wang Yingfei e Dang Yingxu, Objective information theory, Springer, 2023, DOI:10.1007/978-981-19-9929-1, ISBN 978-981-19-9929-1, ISSN 2191-5776.
 Giampio Bracchi, Chiara Francalaci e Gianmario Motta, Sistemi informativi d'impresa, 2ª ed., Milano, McGraw-Hill, 2010, ISBN 978-88-386-6328-4.
 Paolo Atzeni, Stefano Ceri, Piero Fraternali, Stefano Paraboschi e Riccardo Torlone, Basi di dati, 5ª ed., Milano, McGraw-Hill, 2018, ISBN 978-88-386-9445-5.
Yuval Noah Harari, Nexus: Breve storia delle reti di informazioni dall'età della pietra all'IA, (Nexus: A Brief History of Information Networks from the Stone Age to AI, 2024) , trad. di Marco Piani, Collana Saggi, Milano-Firenze, 2024, ISBN 978-88-301-3799-8.


== Voci correlate ==


== Altri progetti ==

 Wikiquote contiene citazioni sull'informazione
 Wikizionario contiene il lemma di dizionario «informazione»
 Wikimedia Commons contiene immagini o altri file sull'informazione


== Collegamenti esterni ==

 informazione, su Treccani.it – Enciclopedie on line, Istituto dell'Enciclopedia Italiana. 
 Giancarlo Provasi, Informazione, in Enciclopedia delle scienze sociali, Istituto dell'Enciclopedia Italiana, 1991-2001. 
 informazióne, su Vocabolario Treccani, Istituto dell'Enciclopedia Italiana. 
 informazióne / informazióne (informatica), su sapere.it, De Agostini. 
(EN) information, su Enciclopedia Britannica, Encyclopædia Britannica, Inc. 
(EN) Informazione, su Stanford Encyclopedia of Philosophy. 
(EN) Denis Howe, information, in Free On-line Dictionary of Computing. Disponibile con licenza GFDL
 Stanford Encyclopedia of Philosophy - Information, su plato.stanford.edu.
 (EN) Jim Al-Khalili, Harnessing The Power Of Information | Order and Disorder | Spark, su YouTube, Spark, 4 novembre 2019.