A probabilidade é a rama das matemáticas que achega descricións numéricas da facilidade de que ocorra un suceso, ou como de posible é que unha proposición sexa verdadeira. A probabilidade dun suceso é un número entre 0 e 1, onde, falando rápido, 0 indica a imposibilidade do suceso e 1 indica a certeza. Cando maior é a probabilidade dun suceso, é maior a facilidade de que ocorra.
A teoría da probabilidade úsase cumpridamente en áreas como a estatística, a física, a matemática, a ciencia e mais a filosofía para tirar conclusións sobre a probabilidade de sucesos potenciais e a mecánica subxacente de sistemas complexos.


== Etimoloxía ==
A palabra probabilidade deriva do latín probabilitas, que tamén pode significar "probidade", medida da autoridade dunha testemuña nun caso legal en Europa, ás veces relacionada coa nobreza da testemuña. Nun sentido difire moito do significado moderno de probabilidade, que é unha medida do peso da evidencia empírica.


== Interpretacións ==
Cando se trata de experimentos aleatorios e ben definidos nun contexto puramente teórico (como lanzar unha moeda perfecta), as probabilidades pódense describir numericamente polo número de resultados desexados dividido entre o número total de todos os resultados. Por exemplo, tirar unha moeda dúas veces devolverá "dúas caras", "cara-cruz", "cruz-cara" e "cruz-cruz". A probabilidade de obter un resultado de "dúas caras" é 1 de cada 4 resultados ou, numericamente, 
  
    
      
        
          
            1
            4
          
        
      
    
    {\displaystyle {\frac {1}{4}}}
  
, 0.25 ou 25 %. Non obstante, cando se trata de aplicacións prácticas, hai dúas grandes categorías de interpretacións da probabilidade en competencia, con partidarios que opinan de xeito diferente sobre a natureza fundamental da probabilidade:

Os obxectivistas asignan números para describir algún estado de cousas obxectivo ou físico. A versión máis popular da probabilidade obxectiva é a probabilidade frecuentista, que afirma que a probabilidade dun suceso aleatorio denota a frecuencia relativa de aparición do resultado dun experimento cando se repite indefinidamente. Esta interpretación considera a probabilidade como a frecuencia relativa "a longo prazo" dos resultados. Unha modificación disto é a probabilidade de propensión, que interpreta a probabilidade como a tendencia dalgún experimento a producir un determinado resultado, aínda que se faga só unha vez.
Os subxectivistas asignan números por probabilidade subxectiva, é dicir, como un grao de crenza. O grao de crenza interpretouse como "o prezo ao que se compraría ou vendería unha aposta que paga 1 unidade de beneficio se 
  
    
      
        E
      
    
    {\displaystyle E}
  
, 0 se non 
  
    
      
        E
      
    
    {\displaystyle E}
  
". A versión máis popular da probabilidade subxectiva é a probabilidade bayesiana, que inclúe coñecemento experto así como datos experimentais para producir probabilidades. O coñecemento experto está representado por algunha distribución de probabilidades a priori (subxectiva). Estes datos están incorporados a unha función de probabilidade. O produto da función a priori e da función de verosimilitude, cando se normaliza, dá lugar a unha probabilidade posterior que incorpora toda a información coñecida ata a data. Polo teorema de concordancia de Aumann, axentes bayesianos con crenzas a priori similares acabarán con crenzas posteriores similares. Non obstante, crenzas previas suficientemente diferentes poden levar a conclusións diferentes, independentemente da cantidade de información que compartan os axentes.


== Historia ==
O estudo da probabilidade xorde do desexo do ser humano por coñecer con certeza os sucesos que acontecerán no futuro. Por iso a través da historia desenvolveu diferentes enfoques para ter un concepto da probabilidade e determinar os seus valores.
A idea de probabilidade está intimamente ligada á idea do azar e axúdanos a comprender as nosas probabilidades de gañar un xogo de azar ou analizar as enquisas. Pierre Simon Laplace afirmou: "É notable que unha ciencia que comezou con consideracións sobre os xogos de azar chegase a ser o obxecto máis importante do coñecemento humano". Comprender e estudar o azar é indispensable porque a probabilidade é un soporte necesario para tomar decisións en calquera ámbito.
As primeiras formas coñecidas de probabilidade e estatística desenvolvéronas os matemáticos do próximo oriente estudando criptografía entre os séculos VIII e XIII. Al-Khalil (717–786) escribiu o Libro das mensaxes criptográficas, que contén os primeiros usos de permutacións e combinacións para listar todas as palabras árabes con e sen vogais. Al-Kindi (801–873) empregou por primeira vez a inferencia estatística nos seus traballos sobre criptoanálise e análise de frecuencias. Unha importante contribución de Ibn Adlan (1187–1268) foi o tamñao da mostra para empregar a análise de frecuencias.
Á parte dalgunhas consideracións elementais feitas por Girolamo Cardano no século XVI, a doutrina do cálculo de probabilidades data da correspondencia entre Pierre de Fermat e Blaise Pascal (1654). Christiaan Huygens (1657) deulle o tratamento científico coñecido máis temperán ao concepto. e Ars Conjectandi (póstumo, 1713) de Jakob Bernoulli e Doctrine of Chances (1718) de Abraham de Moivre trataron o tema como unha póla das matemáticas..
A teoría dos erros pode considerarse que comezou con Opera Miscellanea (póstumo, 1722) de Roger Cotes, mais unha memoria preparada por Thomas Simpson en 1755 (impresa en 1756) aplicou por primeira vez a teoría para a discusión de erros de observación. A reimpresión (1757) desta memoria expón os axiomas de que os erros positivos e negativos son igualmente probables, e que hai certos límites asignables dentro dos cales se supón que caen todos os erros; discútense os erros continuos e dáse unha curva da probabilidade.
Pierre Simon Laplace (1774) fixo o primeiro intento para deducir unha regra para a combinación de observacións a partir dos principios da teoría das probabilidades. Representou a lei da probabilidade de erro cunha curva 
  
    
      
        y
        =
        ϕ
        (
        x
        )
      
    
    {\displaystyle y=\phi (x)}
  
, sendo 
  
    
      
        x
      
    
    {\displaystyle x}
  
 calquera erro e 
  
    
      
        y
      
    
    {\displaystyle y}
  
 a súa probabilidade, e expuxo tres propiedades desta curva:

é simétrica respecto ao eixe 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
;
o eixe 
  
    
      
        X
      
    
    {\displaystyle X}
  
 é unha asíntota, sendo a probabilidade do erro 
  
    
      
        ∞
      
    
    {\displaystyle \infty }
  
 igual a 0;
a superficie encerrada é 1, facendo certa a existencia dun erro.
Deduciu tamén unha fórmula para a media de tres observación e obtivo en 1781 unha fórmula para a lei de facilidade de erro, termo debido a Lagrange (1774), mais esta fórmula levaba a ecuacións inmanexables. Daniel Bernoulli (1778) introduciu o principio do máximo produto das probabilidades dun sistema de erros concorrentes.
O método de mínimos cadrados débese a Adrien-Marie Legendre (1805), que o introduciu en Nouvelles méthodes pour la détermination des orbites des comètes (Novos métodos para a determinación das órbitas dos cometas). Ignorando a contribución de Legendre, un escritor irlandés-estadounidense, Robert Adrain, editor de "The Analyst" (1808), deduciu a lei de facilidade de erro,

  
    
      
        ϕ
        (
        x
        )
        =
        c
        
          e
          
            −
            
              h
              
                2
              
            
            
              x
              
                2
              
            
          
        
      
    
    {\displaystyle \phi (x)=ce^{-h^{2}x^{2}}}
  

sendo 
  
    
      
        c
      
    
    {\displaystyle c}
  
 e 
  
    
      
        h
      
    
    {\displaystyle h}
  
 constantes que dependen da precisión da observación. Expuxo dúas demostracións, sendo a segunda esencialmente a mesma de John Herschel (1850). Gauss expuxo a primeira demostración que parece que se coñeceu en Europa (a terceira despois da de Adrain) en 1809. Demostracións adicionais foron expostas por aplace (1810, 1812), Gauss (1823), James Ivory (1825, 1826), Hagen (1837), Friedrich Bessel (1838), W. F. Donkin (1844, 1856) e Morgan Crofton (1870). Outras personaxes que contribuíron foron Ellis (1844), De Morgan (1864), Glaisher (1872) e Giovanni Schiaparelli (1875). A fórmula de Peters (1856) para 
  
    
      
        r
      
    
    {\displaystyle r}
  
, o erro probable dunha única observación tamén e moi coñecida.
No século XIX, os autores da teoría xeral incluían a Laplace, Sylvestre Lacroix (1816), Littrow (1833), Adolphe Quetelet (1853), Richard Dedekind (1860), Helmert (1872), Hermann Laurent (1873), Liagre, Didion, e Karl Pearson. Augustus De Morgan e George Boole melloraron a exposición da teoría.
En 1906, Andrei Markov introduciu o concepto de cadeas de Markov, que tiveron un importante papel na teoría de procesos estocásticos e as súas aplicacións. En 1930, Kolmogorov desenvolveu a base axiomática da probabilidade empregando a teoría da medida. Na parte xeométrica foron influentes os colaboradores de The Educational Times (Miller, Crofton, McColl, Wolstenholme, Watson e Artemas Martin).


== Teoría ==

A probabilidade constitúe un importante parámetro na determinación das diversas casualidades obtidas tras unha serie de eventos esperados dentro dun rango estatístico. Existen diversas formas como método abstracto, como a teoría Dempster-Shafer e a teoría da relatividade numérica.
A probabilidade dun suceso denótase coa letra p e exprésase en termos dunha fracción, polo que o valor de p cae entre 0 e 1. Por outra banda, a probabilidade de que un suceso "non ocorra" equivale a 1 menos o valor de p e denótase habitualmente coa letra q

  
    
      
        P
        (
        Q
        )
        =
        1
        −
        P
        (
        E
        )
      
    
    {\displaystyle P(Q)=1-P(E)}
  

Os métodos máis usuais para calcular as probabilidades son a regra da adición, a regra do produto e a distribución binomial.


=== Regra da adición ===
A regra da adición establece que a probabilidade de que ocorra un suceso en particular é igual á suma das probabilidades individuais se os sucesos son mutuamente excluínes, é dicir, non poden ocorrer ao mesmo tempo.
Por un lado, se 
  
    
      
        A
        ∩
        B
        =
        ∅
      
    
    {\displaystyle A\cap B=\varnothing }
  
, é dicir que son mutuamente excluíntes, entón:

  
    
      
        P
        (
        A
        ∪
        B
        )
        =
        P
        (
        A
        )
        +
        P
        (
        B
        )
      
    
    {\displaystyle P(A\cup B)=P(A)+P(B)}
  

Por outra banda, se 
  
    
      
        A
        ∩
        B
        ≠
        ∅
      
    
    {\displaystyle A\cap B\neq \varnothing }
  
, é dicir que non son mutuamente excluíntes, entón:

  
    
      
        P
        (
        A
        ∪
        B
        )
        =
        P
        (
        A
        )
        +
        P
        (
        B
        )
        −
        P
        (
        A
        ∩
        B
        )
      
    
    {\displaystyle P(A\cup B)=P(A)+P(B)-P(A\cap B)}
  

sendo 
  
    
      
        P
        (
        A
        )
        =
      
    
    {\displaystyle P(A)=}
  
 probabilidade de que ocorra o suceso 
  
    
      
        A
      
    
    {\displaystyle A}
  
, 
  
    
      
        P
        (
        B
        )
        =
      
    
    {\displaystyle P(B)=}
  
 probabilidade de que ocorra o suceso 
  
    
      
        B
      
    
    {\displaystyle B}
  
, e 
  
    
      
        P
        (
        A
        ∩
        B
        )
        =
      
    
    {\displaystyle P(A\cap B)=}
  
 probabilidade de que ocorran de xeito simultáneo os sucesos 
  
    
      
        A
      
    
    {\displaystyle A}
  
 e 
  
    
      
        B
      
    
    {\displaystyle B}
  
.
Outra forma de velo é expresar a probabilidade de sucesos mutuamente non excluíntes mediante o sumatorio das probabilidades dun suceso determinado en función doutros sucesos:

  
    
      
        P
        (
        x
        )
        =
        
          ∑
          
            y
          
        
        
          P
          (
          x
          ,
          y
          )
        
        =
        
          ∑
          
            y
          
        
        
          P
          (
          y
          )
          P
          (
          x
          
            |
          
          y
          )
        
      
    
    {\displaystyle P(x)=\sum _{y}{P(x,y)}=\sum _{y}{P(y)P(x|y)}}
  


=== Regra do produto ===
A regra do produto establece que a probabilidade de que ocorran dous ou máis sucesos estatisticamente independentes é igual ao produto das súas probabilidades individuais:

  
    
      
        P
        (
        A
        ∩
        B
        )
        =
        P
        (
        A
        )
        P
        (
        B
        )
      
    
    {\displaystyle P(A\cap B)=P(A)P(B)}
  
 se 
  
    
      
        A
      
    
    {\displaystyle A}
  
 e 
  
    
      
        B
      
    
    {\displaystyle B}
  
 son independentes.

  
    
      
        P
        (
        A
        ∩
        B
        )
        =
        P
        (
        A
        )
        P
        (
        B
        
          |
        
        A
        )
      
    
    {\displaystyle P(A\cap B)=P(A)P(B|A)}
  
 se 
  
    
      
        A
      
    
    {\displaystyle A}
  
 e 
  
    
      
        B
      
    
    {\displaystyle B}
  
 non son independentes.


=== Regra de Laplace ===
A regra de Laplace establece que no caso de que os experimentos dean lugar a sucesos equiprobables, é dicir, que todos teñan a mesma probabilidade, a probabilidade de que ocorra un suceso 
  
    
      
        A
      
    
    {\displaystyle A}
  
 calcúlase:

  
    
      
        P
        (
        A
        )
        =
        
          
            Nº de casos favorables
             Nº de resultados posibles
          
        
      
    
    {\displaystyle P(A)={\frac {\text{Nº de casos favorables}}{\text{ Nº de resultados posibles}}}}
  


=== Probabilidade condicionada ===
A probabilidade condicionada é a probabilidade dun suceso A, se ocorreu certo suceso B. Escríbese 
  
    
      
        P
        (
        A
        ∣
        B
        )
      
    
    {\displaystyle P(A\mid B)}
  
, e lese "probabilidade de A, dado B". Defínese como

  
    
      
        P
        (
        A
        ∣
        B
        )
        =
        
          
            
              P
              (
              A
              ∩
              B
              )
            
            
              P
              (
              B
              )
            
          
        
        .
        
      
    
    {\displaystyle P(A\mid B)={\frac {P(A\cap B)}{P(B)}}.\,}
  

Se 
  
    
      
        P
        (
        B
        )
        =
        0
      
    
    {\displaystyle P(B)=0}
  
 entón 
  
    
      
        P
        (
        A
        ∣
        B
        )
      
    
    {\displaystyle P(A\mid B)}
  
 está formalmente indefinida pola expresión. Neste caso, 
  
    
      
        A
      
    
    {\displaystyle A}
  
 e 
  
    
      
        B
      
    
    {\displaystyle B}
  
 son independentes, xa que 
  
    
      
        P
        (
        A
        ∩
        B
        )
        =
        P
        (
        A
        )
        P
        (
        B
        )
        =
        0
      
    
    {\displaystyle P(A\cap B)=P(A)P(B)=0}
  
.
Por exemplo, nunha bolsa con 2 bólas vermellas e 2 bólas azuis (4 bólas en total), a probabilidade de coller unha bóla vermella é 
  
    
      
        1
        
          /
        
        2
      
    
    {\displaystyle 1/2}
  
; non obstante, ao tomar unha segunda bóla, a probabilidade de que sexa vermella ou azul depende da bóla extraída previamente. Por exemplo, se se colleu unha bóla vermella, a probabilidade de extraer unha segunda bóla vermella sería 
  
    
      
        1
        
          /
        
        3
      
    
    {\displaystyle 1/3}
  
, xa que só hai 1 bóla vermella e 2 azuis. En cambio, se se extraeu unha bóla azul, a probabilidade de coller unha vermella será 
  
    
      
        2
        
          /
        
        3
      
    
    {\displaystyle 2/3}
  
.


=== Distribución binomial ===
Nunha serie de experimentos repetidos de xeito independente un número de veces n, e nos que só hai dúas posibilidades de resultado (éxito/fracaso), a distribución da variable X que conta o número de éxitos segue unha distribución binomial. A probabilidade de que ocorran m éxitos nun experimento repetido n veces é:

  
    
      
        P
        (
        x
        =
        m
        )
        =
        
          
            
              (
            
            
              n
              m
            
            
              )
            
          
        
        
          p
          
            m
          
        
        (
        1
        −
        p
        
          )
          
            n
            −
            m
          
        
      
    
    {\displaystyle P(x=m)={n \choose m}p^{m}(1-p)^{n-m}}
  

onde 
  
    
      
        
          
            
              (
            
            
              n
              m
            
            
              )
            
          
        
        =
        
          
            
              n
              !
            
            
              m
              !
              (
              n
              −
              m
              )
              !
            
          
        
      
    
    {\displaystyle {n \choose m}={\frac {n!}{m!(n-m)!}}}
  
 é o número total de combinacións posíbeis de m elementos nun conxunto de n elementos.


=== Resumo de probabilidades ===


== Aplicacións ==
A teoría da probabilidade aplícase na vida cotiá na avaliación e modelización de riscos. A industria e os mercados de seguros utilizan a ciencia actuarial para determinar o prezo e tomar decisións comerciais. Os gobernos aplican métodos probabilísticos na regulación ambiental (onde se lles chama "análise de vías de dispersión"), na análise de dereitos e na regulación financeira. Con frecuencia miden o benestar usando métodos que son estocásticos por natureza, e escollen que proxectos emprender baseándose en análises estatísticos do seu probable efecto na poboación como un conxunto. Non é correcto dicir que a estatística está incluída no propio modelo, xa que as análises de risco adoitan ser para unha única vez e polo tanto requiren máis modelos de probabilidade fundamentais. Unha lei de números pequenos tende a aplicarse a todas aquelas escollas e percepcións do efecto destas escollas, o que fai da medidas probabilísticas un tema político.
Un exemplo do uso da teoría da probabilidade no comercio dos mercados de materias primas é o efecto da probabilidade percibida de calquera conflito xeneralizado en Oriente Medio sobre os prezos do petróleo, que teñen efectos repentinos na economía no seu conxunto. Unha avaliación dun comerciante de produtos básicos de que unha é probable pode enviar os prezos deses produtos básicos cara a arriba ou cara a abaixo, e leva a outros comerciantes a esa opinión. En consecuencia, as probabilidades non son avaliadas de xeito independente nin necesariamente racional. A teoría das finanzas comportamentais xurdiu para describir o efecto deste pensamento grupal nos prezos, na política e na paz e nos conflitos.
Ademais da avaliación financeira, a probabilidade pódese empregar para analizar tendencias en bioloxía (por exemplo, propagación da doenzas) así como en ecoloxía (por exemplo, os cadros biolóxicos de Punnett). Do mesmo xeito en que ocorre coas finanzas, a avaliación de riscos pode usarse como ferramenta estatística para calcular a probabilidade de que ocorran eventos indesexables e pode axudar na implementación de protocolos para evitar atoparse con ditas circunstancias. A probabilidade úsase para deseñar xogos de azar para que os casinos poidan obter un beneficio garantido, mais ofrecen pagos aos xogadores que son o suficientemente frecuentes como para fomentar o xogo continuado.
Outra aplicación significativa da teoría das probabilidades na vida cotiá é a fiabilidade. Moitos produtos de consumo, como automóbiles e electrónica de consumo, utilizan a teoría da fiabilidade no deseño de produtos para reducir a probabilidade de fallo. A probabilidade de fallo pode influír nas decisións do fabricante sobre a garantía dun produto.
O modelo de linguaxe caché e outros modelos estatísticos da linguaxe que se usan no procesamento de linguaxe natural tamén son exemplos de aplicacións da teoría das probabilidades.


== Relación coa aleatoriedade e probabilidade en mecánica cuántica ==
Nun universo determinista, baseado en conceptos newtonianos, non habería probabilidade se se coñecesen todas as condicións (demo de Laplace), aínda que hai situacións nas que a sensibilidade ás condicións iniciais supera a nosa capacidade para medilas, é dicir, coñecelas. No caso dunha ruleta, se se coñece a forza da man e o período desa forza, o número en que se deterá a pelota sería unha certeza (aínda que, como cuestión práctica, isto probablemente só sería certo cunha ruleta que non estivese exactamente nivelada, como revelou o "casino newtoniano" de Thomas A. Bass). Isto tamén asume o coñecemento da inercia e do rozamento da roda, o peso, a suavidade e a redondez da bóla, as variacións na velocidade da man durante o xiro e así sucesivamente. Unha descrición probabilística pode ser máis útil que a mecánica newtoniana para analizar o patrón de resultados repetidos dunha ruleta. Os físicos enfróntanse á mesma situación na teoría cinética dos gases, onde o sistema, aínda que é determinista en principio, é tan complexo (co número de moléculas normalmente de orde de magnitude da constante de Avogadro 6,02×1023) que só unha descrición estatística das súas propiedades é factible.
A teoría da probabilidade é necesaria para describir os fenómenos cuánticos. Un descubrimento revolucionario da física de principios do século XX foi o carácter aleatorio de todos os procesos físicos que se producen a escalas subatómicas e que se rexen polas leis da mecánica cuántica. A función de onda obxectiva evoluciona de xeito determinista mais, segundo a interpretación de Copenhaguen, trata de probabilidades de observación, explicándose o resultado por un colapso da función de onda cando se fai unha observación. Non obstante, a perda de determinismo co fin do instrumentalismo non atopou a aprobación universal. Albert Einstein comentou famosamente nunha carta a Max Born: "Estou convencido de que Deus non xoga aos dados". Como Einstein, Erwin Schrödinger, que descubriu a función de onda, cría que a mecánica cuántica é unha aproximación estatística dunha realidade determinista subxacente. Nalgunhas interpretacións modernas da mecánica estatística da medida, invócase a descoherencia cuántica para dar conta da aparición de resultados experimentais subxectivamente probabilísticos.


== Notas ==


== Véxase tamén ==


=== Bibliografía ===
Kallenberg, O. (2005) Probabilistic Symmetries and Invariance Principles. Springer-Verlag, Nova York. 510 pp. ISBN 0-387-25115-4
Kallenberg, O. (2002) Foundations of Modern Probability, 2nd ed. Springer Series in Statistics. 650 pp. ISBN 0-387-95313-2
Olofsson, Peter (2005) Probability, Statistics, and Stochastic Processes, Wiley-Interscience. 504 pp ISBN 0-471-67969-0.


=== Ligazóns externas ===
Historia da probabilidade (en castelán)